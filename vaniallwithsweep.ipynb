{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-13T17:42:35.141833Z","iopub.status.busy":"2023-05-13T17:42:35.141435Z","iopub.status.idle":"2023-05-13T17:42:35.150023Z","shell.execute_reply":"2023-05-13T17:42:35.148872Z","shell.execute_reply.started":"2023-05-13T17:42:35.141791Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from datetime import datetime\n","import wandb\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:39.670865Z","iopub.status.busy":"2023-05-13T17:42:39.670480Z","iopub.status.idle":"2023-05-13T17:42:42.237395Z","shell.execute_reply":"2023-05-13T17:42:42.236056Z","shell.execute_reply.started":"2023-05-13T17:42:39.670829Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key='44fa96b263794ea519fb29399eb6b8f469eb934b')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:42.240658Z","iopub.status.busy":"2023-05-13T17:42:42.240271Z","iopub.status.idle":"2023-05-13T17:42:42.246553Z","shell.execute_reply":"2023-05-13T17:42:42.245385Z","shell.execute_reply.started":"2023-05-13T17:42:42.240613Z"},"trusted":true},"outputs":[],"source":["##44fa96b263794ea519fb29399eb6b8f469eb934b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:42.249226Z","iopub.status.busy":"2023-05-13T17:42:42.248111Z","iopub.status.idle":"2023-05-13T17:42:42.430893Z","shell.execute_reply":"2023-05-13T17:42:42.429754Z","shell.execute_reply.started":"2023-05-13T17:42:42.249182Z"},"trusted":true},"outputs":[],"source":["train_dataset = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv',header=None)\n","test_dataset=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_test.csv',header=None)\n","valid_dataset=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_valid.csv' ,header=None)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:42.460868Z","iopub.status.busy":"2023-05-13T17:42:42.460518Z","iopub.status.idle":"2023-05-13T17:42:42.466391Z","shell.execute_reply":"2023-05-13T17:42:42.465274Z","shell.execute_reply.started":"2023-05-13T17:42:42.460835Z"},"trusted":true},"outputs":[],"source":["lang2_ = 'HINDI'\n","SOW_token = 0 #<\n","EOW_token = 1 #>\n","UNK_token = 2 #_\n","PAD_token = 3 #@\n","#fix_this \n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:42.878006Z","iopub.status.busy":"2023-05-13T17:42:42.877326Z","iopub.status.idle":"2023-05-13T17:42:42.885676Z","shell.execute_reply":"2023-05-13T17:42:42.884555Z","shell.execute_reply.started":"2023-05-13T17:42:42.877968Z"},"trusted":true},"outputs":[],"source":["class make_dict:\n","    def __init__(self,name):\n","        self.name=name\n","        self.chartoindex={}\n","        self.indextochar={0: \"<\", 1: \">\",2:\"_\",3:'@'}\n","        self.n_chars=4\n","    \n","    def addchar(self,word):\n","        for char in word:\n","            if char not in self.chartoindex:\n","                self.chartoindex[char]=self.n_chars\n","                self.indextochar[self.n_chars]=char\n","                self.n_chars+=1\n","            "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:43.338296Z","iopub.status.busy":"2023-05-13T17:42:43.337434Z","iopub.status.idle":"2023-05-13T17:42:43.346750Z","shell.execute_reply":"2023-05-13T17:42:43.345534Z","shell.execute_reply.started":"2023-05-13T17:42:43.338254Z"},"trusted":true},"outputs":[],"source":["# doing it on train a dataframe is given as input having input output \n","# and it make the disctionary based on that and return input_dict_obj,outout_dict_obj\n","#and the pair's as input ,output\n","def Input_Output_train(df):\n","    #convert the dataframe to list \n","    input_word=df.iloc[:,0].tolist()\n","    output_word=df.iloc[:,1].tolist()\n","    input_lang=make_dict('ENG')\n","    output_lang=make_dict('HINDI')\n","    \n","    # to make a dictionary of characters for the input language \n","    for word in input_word:\n","        input_lang.addchar(word)\n","   # to make a dictionary of characters for teh output language \n","    for word in output_word:\n","        output_lang.addchar(word)\n","    return input_lang,output_lang,input_word,output_word\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:44.274032Z","iopub.status.busy":"2023-05-13T17:42:44.272945Z","iopub.status.idle":"2023-05-13T17:42:44.281744Z","shell.execute_reply":"2023-05-13T17:42:44.280388Z","shell.execute_reply.started":"2023-05-13T17:42:44.273993Z"},"trusted":true},"outputs":[],"source":["#use to get max length of the input dataframe provided \n","def getlength_x(df):\n","    input_list = df.iloc[:,0].tolist()\n","    input_list_length=[]\n","    for word in input_list:\n","        input_list_length.append(len(word))\n","    \n","    input_max_length=max(input_list_length)\n","    return input_max_length+1\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:44.784466Z","iopub.status.busy":"2023-05-13T17:42:44.783450Z","iopub.status.idle":"2023-05-13T17:42:44.790981Z","shell.execute_reply":"2023-05-13T17:42:44.789881Z","shell.execute_reply.started":"2023-05-13T17:42:44.784415Z"},"trusted":true},"outputs":[],"source":["#to get max length of the output \n","def getlength_y(df):\n","    output_list =df.iloc[:,1].tolist()\n","    output_list_length=[]\n","    for word in output_list:\n","        output_list_length.append(len(word))\n","    output_max_length=max(output_list_length)\n","    return output_max_length+1\n","    \n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:45.216989Z","iopub.status.busy":"2023-05-13T17:42:45.216586Z","iopub.status.idle":"2023-05-13T17:42:45.343790Z","shell.execute_reply":"2023-05-13T17:42:45.342755Z","shell.execute_reply.started":"2023-05-13T17:42:45.216952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ENG 30\n","HINDI 68\n"]}],"source":["input_word_dict,output_word_dict,inputword_list,outputword_list=Input_Output_train(train_dataset)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:46.437030Z","iopub.status.busy":"2023-05-13T17:42:46.435747Z","iopub.status.idle":"2023-05-13T17:42:46.443165Z","shell.execute_reply":"2023-05-13T17:42:46.442045Z","shell.execute_reply.started":"2023-05-13T17:42:46.436976Z"},"trusted":true},"outputs":[],"source":["def getlistofwords_x(df):\n","    return df.iloc[:,0].tolist()\n","\n","def getlistofwords_y(df):\n","    return df.iloc[:,1].tolist()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:46.886494Z","iopub.status.busy":"2023-05-13T17:42:46.885742Z","iopub.status.idle":"2023-05-13T17:42:46.891667Z","shell.execute_reply":"2023-05-13T17:42:46.890481Z","shell.execute_reply.started":"2023-05-13T17:42:46.886453Z"},"trusted":true},"outputs":[],"source":["valid_input_list = getlistofwords_x(valid_dataset)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:47.529011Z","iopub.status.busy":"2023-05-13T17:42:47.528213Z","iopub.status.idle":"2023-05-13T17:42:47.535874Z","shell.execute_reply":"2023-05-13T17:42:47.534772Z","shell.execute_reply.started":"2023-05-13T17:42:47.528967Z"},"trusted":true},"outputs":[],"source":["#to form tensor list pass the wordlist,maxlength,dictusing \n","def tensorsFormation(inputword,max_length,input_dict):\n","    #convering the input and output to tensor\n","    \n","    inlist=[]\n","    for char in inputword:\n","        inlist.append(input_dict.chartoindex[char])\n","    inlist.append(EOW_token)\n","    diff=max_length-len(inlist)\n","    inlist.extend([PAD_token]*diff)\n","    wordtensor=torch.tensor(inlist, dtype=torch.long,device=device)\n","        \n","    \n","    return wordtensor"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:48.132926Z","iopub.status.busy":"2023-05-13T17:42:48.132552Z","iopub.status.idle":"2023-05-13T17:42:48.138719Z","shell.execute_reply":"2023-05-13T17:42:48.137533Z","shell.execute_reply.started":"2023-05-13T17:42:48.132890Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:48.471973Z","iopub.status.busy":"2023-05-13T17:42:48.470726Z","iopub.status.idle":"2023-05-13T17:42:48.478914Z","shell.execute_reply":"2023-05-13T17:42:48.477535Z","shell.execute_reply.started":"2023-05-13T17:42:48.471922Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:48.908811Z","iopub.status.busy":"2023-05-13T17:42:48.907644Z","iopub.status.idle":"2023-05-13T17:42:48.916889Z","shell.execute_reply":"2023-05-13T17:42:48.915638Z","shell.execute_reply.started":"2023-05-13T17:42:48.908760Z"},"trusted":true},"outputs":[],"source":["def batchcreation(data,batchsize):\n","    size=len(data)\n","    #print(size)\n","    batchdata=[]\n","    for i in range(0,size,batchsize):\n","        temp=torch.stack(data[i:i+batchsize]).to(device)\n","        batchdata.append(temp.transpose(0,1))\n","        #batchdata.append((nn.utils.rnn.pad_sequence(data[i:i+batch_size])).to(device))\n","    \n","    return batchdata"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:50.199671Z","iopub.status.busy":"2023-05-13T17:42:50.198558Z","iopub.status.idle":"2023-05-13T17:42:50.204816Z","shell.execute_reply":"2023-05-13T17:42:50.203552Z","shell.execute_reply.started":"2023-05-13T17:42:50.199628Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:52.522571Z","iopub.status.busy":"2023-05-13T17:42:52.521596Z","iopub.status.idle":"2023-05-13T17:42:52.545347Z","shell.execute_reply":"2023-05-13T17:42:52.544144Z","shell.execute_reply.started":"2023-05-13T17:42:52.522514Z"},"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, parameters):\n","        super(EncoderRNN, self).__init__()\n","        self.input_size=parameters['input_size_encoder']\n","        self.dropout = nn.Dropout(parameters['dropout_encoder'])\n","        self.dropout_val=parameters['dropout_encoder']\n","        self.num_layers = parameters['num_layers']\n","        self.batch_size = parameters['batch_size']\n","        self.embedding_size = parameters['embedding_size']\n","        self.hidden_size = parameters['hidden_size']\n","        self.cell_type = parameters['cell_type']\n","        self.embedding = nn.Embedding(parameters['input_size_encoder'], parameters['embedding_size'])\n","        self.bidirection = parameters['bidirectional']\n","        \n","        if(self.cell_type == \"GRU\" ):\n","            self.cell_calc = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val, bidirectional = self.bidirection)\n","        elif(self.cell_type == \"LSTM\"):\n","            self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val, bidirectional = self.bidirection)\n","        elif(self.cell_type == \"RNN\"):\n","            self.cell_calc = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val, bidirectional = self.bidirection)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n","        output = self.dropout(embedded)\n","        if(self.cell_type == \"LSTM\"):\n","            _,(hidden, cell) = self.lstm(output)\n","        else:\n","            _,hidden = self.cell_calc(output, hidden)\n","                    \n","        if (self.bidirection==True):\n","            #rehsaping the hidden layer to calculate the avg of the two layers \n","            \n","            dim1=hidden.size(1)\n","            dim2=hidden.size(2)\n","            hidden = hidden.reshape(2, -1, dim1, dim2)\n","            #print(hidden.shape)\n","            temp = hidden[0]\n","            temp.add(hidden[1])\n","            hidden=0.5*temp\n","            \n","            \n","            if(self.cell_type == \"LSTM\"):\n","                dim1= cell.size(1)\n","                dim2= cell.size(2)\n","                cell = cell.reshape(2,-1,dim1,dim2)\n","                temp=cell[0]\n","                temp.add(cell[1])\n","                cell=0.5*temp\n","        if self.cell_type == \"LSTM\":\n","            return hidden, cell\n","        else:\n","            return hidden\n","\n","    def initHidden(self):\n","        if self.bidirection:\n","            return torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size).to(device)\n","        else:\n","            return torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(device)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:53.506219Z","iopub.status.busy":"2023-05-13T17:42:53.505803Z","iopub.status.idle":"2023-05-13T17:42:53.523686Z","shell.execute_reply":"2023-05-13T17:42:53.522313Z","shell.execute_reply.started":"2023-05-13T17:42:53.506181Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, parameters):\n","        super(DecoderRNN, self).__init__()\n","        self.output_size=parameters['output_size_decoder']\n","        self.dropout = nn.Dropout(parameters['dropout_decoder'])\n","        self.dropout_val=parameters['dropout_decoder']\n","        self.num_layers = parameters['num_layers']\n","        self.batch_size = parameters['batch_size']\n","        self.embedding_size = parameters['embedding_size']\n","        self.hidden_size = parameters['hidden_size']\n","        self.cell_type = parameters['cell_type']\n","        self.embedding = nn.Embedding(parameters['output_size_decoder'], parameters['embedding_size'])\n","               \n","       \n","        if(self.cell_type == \"GRU\"):\n","            self.cell_calc = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val)\n","        elif(self.cell_type == \"RNN\"):\n","            self.cell_calc = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val)\n","        elif(self.cell_type == \"LSTM\"):\n","            self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = self.dropout_val)\n","        \n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n","        output = self.dropout(output)\n","        output = torch.relu(output)\n","        if(self.cell_type == \"LSTM\"):\n","            output, (hidden, cell) = self.lstm(output, (hidden[0], hidden[1]))\n","        else:\n","            output, hidden = self.cell_calc(output, hidden)\n","\n","        if self.cell_type == \"LSTM\":\n","            return self.softmax(self.out(output[0])),hidden,cell\n","        return self.softmax(self.out(output[0])),hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:54.477897Z","iopub.status.busy":"2023-05-13T17:42:54.477512Z","iopub.status.idle":"2023-05-13T17:42:54.493517Z","shell.execute_reply":"2023-05-13T17:42:54.492158Z","shell.execute_reply.started":"2023-05-13T17:42:54.477861Z"},"trusted":true},"outputs":[],"source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,train_iter_params):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","    \n","    loss = 0\n","    if train_iter_params['cell_type'] == \"LSTM\":\n","        encoder_hidden, encoder_cell = encoder(input_tensor, encoder_hidden)\n","    else:\n","        encoder_hidden = encoder(input_tensor, encoder_hidden)\n","        \n","    decoder_input = torch.tensor([SOW_token]*train_iter_params['batch_size']).to(device)\n","    \n","#     print(encoder_output.shape, encoder_hidden.shape)\n","\n","    decoder_hidden = encoder_hidden\n","    if train_iter_params['cell_type'] == \"LSTM\":\n","        decoder_cell = encoder_cell\n","\n","    if(random.random() < teacher_forcing_ratio):\n","        use_teacher_forcing = True \n","    else:\n","        use_teacher_forcing=False\n","        \n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            if train_iter_params['cell_type'] == \"LSTM\":\n","                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n","            else:\n","                #print(decoder_input.shape,decoder_hidden.shape)\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            if train_iter_params['cell_type'] == \"LSTM\":\n","                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n","            else:\n","                #print(decoder_input.shape,decoder_hidden.shape)\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T18:39:12.254068Z","iopub.status.busy":"2023-05-13T18:39:12.253637Z","iopub.status.idle":"2023-05-13T18:39:12.275660Z","shell.execute_reply":"2023-05-13T18:39:12.274486Z","shell.execute_reply.started":"2023-05-13T18:39:12.254031Z"},"trusted":true},"outputs":[],"source":["def train_setup(encoder, decoder, train_iter_params,parameters):\n","    \n","    \n","    Total_loss = 0  # Reset every print_every\n","    run_name = \"EBS_{}_NL_{}_HL_{}_CellT_{}_BS_{}\".format(parameters['embedding_size'], parameters['num_layers'], parameters['hidden_size'], parameters['cell_type'], parameters['batch_size'])\n","    \n","    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=train_iter_params['learning_rate'], weight_decay = 0.0005)\n","    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=train_iter_params['learning_rate'], weight_decay = 0.0005)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    input_tensor_list=[]\n","\n","    for i in range(len(inputword_list)):\n","        if(i%train_iter_params['batch_size']==0):\n","            max_length_x=getlength_x(train_dataset[i:i+train_iter_params['batch_size']])\n","\n","        inputwordtensor =tensorsFormation(inputword_list[i],max_length_x,input_word_dict)\n","        input_tensor_list.append(inputwordtensor)\n","    \n","    output_tensor_list=[]\n","\n","    for i in range(len(outputword_list)):\n","        if(i%train_iter_params['batch_size']==0):\n","            max_length_y=getlength_y(train_dataset[i:i+train_iter_params['batch_size']])\n","        outputwordtesor = tensorsFormation(outputword_list[i],max_length_y,output_word_dict)\n","        output_tensor_list.append(outputwordtesor)\n","\n","    valid_iptensor_list=[]\n","\n","    for i in range(len(valid_input_list)):\n","        if(i%train_iter_params['batch_size']==0):\n","            valid_max_length_x=getlength_x(valid_dataset[i:i+train_iter_params['batch_size']])\n","        validipwordtensor =tensorsFormation(valid_input_list[i],valid_max_length_x,input_word_dict)\n","        valid_iptensor_list.append(validipwordtensor)\n","\n","    \n","    \n"," #   input_tensor_list=creatensorlist_x(train_dataset,inputword_list,parameters['batch_size'],input_word_dict)\n","#     output_tensor_list=creatensorlist_y(train_dataset,outputword_list,train_iter_params['batch_size'],output_word_dict)\n","#     valid_iptensor_list=creatensorlist_x(valid_dataset,valid_input_list,train_iter_params['batch_size'],input_word_dict)\n","    \n","    \n","    train_batch_input = batchcreation(input_tensor_list, train_iter_params['batch_size'])\n","    train_batch_target = batchcreation(output_tensor_list, train_iter_params['batch_size'])\n","    valid_batch_input = batchcreation(valid_iptensor_list, train_iter_params['batch_size'])\n","    no_of_batches=len(train_batch_target)\n","    no_of_datapoints=train_iter_params['batch_size']*no_of_batches\n","   \n","        \n","    for count in range(train_iter_params['epochs']):\n","        #print(\"Epoch \", count ,\" started \", datetime.now())\n","        print('epoch count is ',count+1 ,'time' ,datetime.now())\n","        for i in range(no_of_batches):\n","            loss = train(train_batch_input[i], train_batch_target[i], encoder,\n","                         decoder, encoder_optimizer, decoder_optimizer, criterion,train_iter_params)\n","            Total_loss += loss*train_iter_params['batch_size']\n","        \n","        \n","\n","        avg_loss = Total_loss/no_of_datapoints\n","        Total_loss = 0\n","        print(\"Average loss after \", count+1, \"epochs is \", avg_loss)\n","#        train_accuracy = cal_Accuracy(train_batch_input, train_output, cell_type, len(train_input), batch_size)\n","#         print(\"Train accuracy is \", train_accuracy)\n","\n","        valid_output_list = valid_dataset.iloc[:,1]\n","        valid_accuracy = cal_Accuracy(valid_batch_input, valid_output_list, train_iter_params['cell_type'], train_iter_params['batch_size'],encoder,decoder)\n","        print(\"Valid accuracy is \", valid_accuracy)\n","        wandb.log({\"validation_accuracy\": valid_accuracy, \"training_loss\": avg_loss, 'epoch': count})\n","    wandb.run.name = run_name\n","    wandb.run.save()\n","    wandb.run.finish()\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:55.769351Z","iopub.status.busy":"2023-05-13T17:42:55.768507Z","iopub.status.idle":"2023-05-13T17:42:55.782615Z","shell.execute_reply":"2023-05-13T17:42:55.781515Z","shell.execute_reply.started":"2023-05-13T17:42:55.769311Z"},"trusted":true},"outputs":[],"source":["def evaluate(encoder, decoder, input_tensors, cell_type,batch_size):\n","    with torch.no_grad():\n","        \n","        input_length = input_tensors.size(0)\n","        predicted_word_tensor=[]\n","        encoder_hidden = encoder.initHidden()\n","\n","        if cell_type == \"LSTM\":\n","            encoder_hidden, encoder_cell = encoder(input_tensors, encoder_hidden)\n","        else:\n","            encoder_hidden = encoder(input_tensors, encoder_hidden)\n","\n","        decoder_input = torch.tensor([SOW_token]*batch_size, device=device)  # SOW\n","\n","        decoder_hidden = encoder_hidden\n","        if cell_type == \"LSTM\":\n","            decoder_cell = encoder_cell\n","            \n","        for di in range(input_length):\n","            \n","            if cell_type == \"LSTM\":\n","                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n","            else:\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.data.topk(1)\n","                                    \n","            temp=topi.view(1,-1).squeeze()\n","            predicted_word_tensor.append(temp)\n","            decoder_input = topi.squeeze().detach()\n","            \n","        predicted_word_tensor=torch.stack(predicted_word_tensor,dim=1).to(device)\n","        \n","        return predicted_word_tensor\n","        "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:56.338437Z","iopub.status.busy":"2023-05-13T17:42:56.337710Z","iopub.status.idle":"2023-05-13T17:42:56.348724Z","shell.execute_reply":"2023-05-13T17:42:56.347469Z","shell.execute_reply.started":"2023-05-13T17:42:56.338398Z"},"trusted":true},"outputs":[],"source":["def cal_Accuracy(input, actual_output, cell_type, batch_size,encoder1,decoder1):\n","    correct_output=0\n","    no_of_batch=len(input)\n","    no_of_datapoints=batch_size*no_of_batch \n","    incorrect_correct_pair=[]\n","    \n","    for i in range(no_of_batch):\n","        output_tensor_predicted = evaluate(encoder1, decoder1, input[i], cell_type,batch_size)\n","                \n","        for j in range(output_tensor_predicted.size(0)):\n","            string=\"\"\n","            for k in range(output_tensor_predicted.size(1)):\n","                target_val=output_tensor_predicted[j][k].item()\n","                if(target_val==EOW_token or target_val==3):\n","                    break\n","                else:\n","                    string =string+output_word_dict.indextochar[target_val]\n","            \n","            #print(string)\n","            if(actual_output[i*batch_size+j]==string):\n","                correct_output+=1\n","           # else:\n","           #     incorrect_correct_pair.append([])\n","                \n","                \n","              \n","    #print('no of correct output =',correct_output/n *100)\n","    \n","    return correct_output/no_of_datapoints *100\n","            \n","  \n","    "]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:57.147648Z","iopub.status.busy":"2023-05-13T17:42:57.146265Z","iopub.status.idle":"2023-05-13T17:42:57.154620Z","shell.execute_reply":"2023-05-13T17:42:57.153272Z","shell.execute_reply.started":"2023-05-13T17:42:57.147604Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:58.012408Z","iopub.status.busy":"2023-05-13T17:42:58.011172Z","iopub.status.idle":"2023-05-13T17:42:58.017903Z","shell.execute_reply":"2023-05-13T17:42:58.016333Z","shell.execute_reply.started":"2023-05-13T17:42:58.012356Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:58.746890Z","iopub.status.busy":"2023-05-13T17:42:58.745922Z","iopub.status.idle":"2023-05-13T17:42:58.752650Z","shell.execute_reply":"2023-05-13T17:42:58.751233Z","shell.execute_reply.started":"2023-05-13T17:42:58.746835Z"},"trusted":true},"outputs":[],"source":["# paramet= {\n","\n","# \"input_size_encoder\":input_word_dict.n_chars,\n","# \"output_size_decoder\":output_word_dict.n_chars,\n","# \"hidden_size\":  512,\n","# \"num_layers\":  2,\n","# \"dropout_encoder\":0.4,\n","# \"dropout_decoder\": 0.3,\n","# \"batch_size\": 32,\n","# \"embedding_size\": 256,\n","# \"epochs\": 10,\n","# \"cell_type\":\"RNN\",\n","# \"bidirectional\": True\n","    \n","# }"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:59.308082Z","iopub.status.busy":"2023-05-13T17:42:59.306761Z","iopub.status.idle":"2023-05-13T17:42:59.313538Z","shell.execute_reply":"2023-05-13T17:42:59.312347Z","shell.execute_reply.started":"2023-05-13T17:42:59.308031Z"},"trusted":true},"outputs":[],"source":["# encodertest = EncoderRNN(paramet).to(device)\n","# decodertest = DecoderRNN(paramet).to(device)\n","\n","# train_iter_params={\n","#     'epochs':paramet['epochs'],\n","#     'learning_rate':0.001,\n","#     'batch_size':paramet['batch_size'],\n","#     'cell_type':paramet['cell_type']  \n","    \n","    \n","#  }\n","# train_setup(encodertest, decodertest, train_iter_params,paramet)\n","    "]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:42:59.953709Z","iopub.status.busy":"2023-05-13T17:42:59.953011Z","iopub.status.idle":"2023-05-13T17:42:59.965493Z","shell.execute_reply":"2023-05-13T17:42:59.964161Z","shell.execute_reply.started":"2023-05-13T17:42:59.953670Z"},"trusted":true},"outputs":[],"source":["def runSweep():\n","    \n","    config_defaults = {\n","        \"embedding_size\": 64,\n","        \"num_layers\": 3,\n","        \"hidden_layer\": 256,\n","        \"cell_type\": \"LSTM\",\n","        \"bidirectional\": True,\n","        \"dropout_encoder\": 0.2,\n","        \"dropout_decoder\": 0.3,\n","        \"epochs\": 20,\n","        \"batch_size\": 32,\n","    }\n","    wandb.init( entity = 'singhbhavesh999', project = 'DLSeqtoseq',config=config_defaults)\n","    \n","    param= {\n","\n","        \"input_size_encoder\":input_word_dict.n_chars,\n","        \"output_size_decoder\":output_word_dict.n_chars,\n","        \"hidden_size\":  wandb.config.hidden_layer,\n","        \"num_layers\":  wandb.config.num_layers,\n","        \"dropout_encoder\":wandb.config.dropout_encoder,\n","        \"dropout_decoder\": wandb.config.dropout_decoder,\n","        \"batch_size\": wandb.config.batch_size,\n","        \"embedding_size\": wandb.config.embedding_size,\n","        \"epochs\": wandb.config.epochs,\n","        \"cell_type\":wandb.config.cell_type,\n","        \"bidirectional\": wandb.config.bidirectional\n","\n","        }\n","    \n","\n","\n","    \n","    encoder1 = EncoderRNN(param).to(device)\n","    decoder1 = DecoderRNN(param).to(device)\n","    train_iter_params={\n","        'epochs':param['epochs'],\n","        'learning_rate':0.001,\n","        'batch_size':param['batch_size'],\n","        'cell_type':param['cell_type']  \n","    }\n","    \n"," \n","    train_setup(encoder1, decoder1, train_iter_params,param)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T19:02:48.516630Z","iopub.status.busy":"2023-05-13T19:02:48.516135Z","iopub.status.idle":"2023-05-13T19:02:56.700197Z","shell.execute_reply":"2023-05-13T19:02:56.699044Z","shell.execute_reply.started":"2023-05-13T19:02:48.516583Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: xl00ecq9\n","Sweep URL: https://wandb.ai/singhbhavesh999/DLSeqtoseq/sweeps/xl00ecq9\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ad4cdmho with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_decoder: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_encoder: 0.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 1144, in init\n","    run = wi.init()\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 801, in init\n","    run_start_result = run_start_handle.wait(timeout=30)\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 283, in wait\n","    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 130, in _get_and_clear\n","    if self._wait(timeout=timeout):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 126, in _wait\n","    return self._event.wait(timeout=timeout)\n","  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n","    signaled = self._cond.wait(timeout)\n","  File \"/opt/conda/lib/python3.7/threading.py\", line 300, in wait\n","    gotit = waiter.acquire(True, timeout)\n","Exception\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 1152, in init\n","    getcaller()\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 838, in getcaller\n","    src, line, func, stack = logger.findCaller(stack_info=True)\n","  File \"/root/.local/lib/python3.7/site-packages/log.py\", line 42, in findCaller\n","    sio = io.StringIO()\n","NameError: name 'io' is not defined\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"]}],"source":["sweep_config = {\n","  \n","  \"metric\": {\n","      \"name\":\"validation_accuracy\",\n","      \"goal\": \"maximize\"\n","  },\n","  \"method\": \"bayes\",\n","  \"parameters\": {\n","        \"embedding_size\": {\n","            \"values\": [512, 256, 64, 32]\n","        },\n","        \"num_layers\": {\n","            \"values\": [3, 2, 1]\n","        },\n","        \"hidden_layer\": {\n","            \"values\": [512, 256, 128]\n","        },\n","        \"cell_type\": {\n","            \"values\": [\"RNN\",\"GRU\",\"LSTM\"]\n","        },\n","        \"bidirectional\": {\n","            \"values\": [True,False]\n","        },\n","        \"dropout_encoder\": {\n","            \"values\": [0.2, 0.3, 0.4]\n","        },\n","        \"dropout_decoder\": {\n","            \"values\": [0.2, 0.3, 0.4]\n","        },\n","        \"epochs\": {\n","            \"values\": [10,15]\n","        },\n","        \"batch_size\": {\n","            \"values\": [256, 128, 64, 32]\n","        }\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep_config, entity=\"singhbhavesh999\", project=\"DLSeqtoseq\")\n","wandb.agent(sweep_id, runSweep, count = 100)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T17:17:11.553176Z","iopub.status.busy":"2023-05-13T17:17:11.551918Z","iopub.status.idle":"2023-05-13T17:17:11.562178Z","shell.execute_reply":"2023-05-13T17:17:11.561028Z","shell.execute_reply.started":"2023-05-13T17:17:11.553137Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'embedding_size': {'values': [512, 256, 64, 32]},\n"," 'num_layers': {'values': [3, 2, 1]},\n"," 'hidden_layer': {'values': [512, 256, 128]},\n"," 'cell_type': {'values': ['RNN']},\n"," 'bidirectional': {'values': [True]},\n"," 'dropout_encoder': {'values': [0.2, 0.3, 0.4]},\n"," 'dropout_decoder': {'values': [0.2, 0.3, 0.4]},\n"," 'epochs': {'values': [10]},\n"," 'batch_size': {'values': [256, 128, 64, 32]}}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T18:16:26.348348Z","iopub.status.busy":"2023-05-13T18:16:26.347479Z","iopub.status.idle":"2023-05-13T18:16:26.353548Z","shell.execute_reply":"2023-05-13T18:16:26.352448Z","shell.execute_reply.started":"2023-05-13T18:16:26.348294Z"},"trusted":true},"outputs":[],"source":["# wandb.agent('a0km7f3z', runSweep, count = 2)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:32:36.239554Z","iopub.status.busy":"2023-05-13T14:32:36.239090Z","iopub.status.idle":"2023-05-13T14:32:36.287248Z","shell.execute_reply":"2023-05-13T14:32:36.284494Z","shell.execute_reply.started":"2023-05-13T14:32:36.239505Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'batch_size' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3230542203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 1144, in init\n","    run = wi.init()\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 801, in init\n","    run_start_result = run_start_handle.wait(timeout=30)\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 283, in wait\n","    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 130, in _get_and_clear\n","    if self._wait(timeout=timeout):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 126, in _wait\n","    return self._event.wait(timeout=timeout)\n","  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n","    signaled = self._cond.wait(timeout)\n","  File \"/opt/conda/lib/python3.7/threading.py\", line 300, in wait\n","    gotit = waiter.acquire(True, timeout)\n","Exception\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 1152, in init\n","    getcaller()\n","  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\", line 838, in getcaller\n","    src, line, func, stack = logger.findCaller(stack_info=True)\n","  File \"/root/.local/lib/python3.7/site-packages/log.py\", line 42, in findCaller\n","    sio = io.StringIO()\n","NameError: name 'io' is not defined\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
